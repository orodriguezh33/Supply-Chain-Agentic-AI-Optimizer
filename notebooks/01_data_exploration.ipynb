{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio ra√≠z al path para encontrar src\n",
    "# Esto asume que el notebook est√° en la carpeta 'notebooks/'\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# --- AQU√ç EST√Å EL CAMBIO ---\n",
    "# En lugar de 'from src.utils.db import get_db'\n",
    "from src.utils.db import SupplyChainDB \n",
    "\n",
    "# Configuraci√≥n visual\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Inicializar la conexi√≥n\n",
    "db = SupplyChainDB()\n",
    "print(\"‚úÖ Conexi√≥n establecida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbd383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Agregar el directorio padre al path de Python (para encontrar src)\n",
    "sys.path.append('..')\n",
    "\n",
    "# 2. CAMBIO CR√çTICO: Mover el contexto de ejecuci√≥n a la ra√≠z del proyecto\n",
    "# Esto hace que las rutas relativas 'data/processed/...' funcionen correctamente\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "print(f\"üìÇ Directorio de trabajo actual: {os.getcwd()}\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "\n",
    "from src.utils.db import SupplyChainDB\n",
    "\n",
    "# Setup\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Load database (Ahora s√≠ encontrar√° el archivo lleno)\n",
    "db = SupplyChainDB()\n",
    "\n",
    "# Load baseline metrics\n",
    "with open('data/processed/baseline_metrics.json', 'r') as f: # Nota: quit√© el '../' porque ya estamos en root\n",
    "    baseline = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Agregar el directorio padre al path de Python (para encontrar src)\n",
    "sys.path.append('..')\n",
    "\n",
    "# 2. CAMBIO CR√çTICO: Mover el contexto de ejecuci√≥n a la ra√≠z del proyecto\n",
    "# Esto hace que las rutas relativas 'data/processed/...' funcionen correctamente\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "print(f\"üìÇ Directorio de trabajo actual: {os.getcwd()}\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "\n",
    "from src.utils.db import SupplyChainDB\n",
    "\n",
    "# Setup\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Load database (Ahora s√≠ encontrar√° el archivo lleno)\n",
    "db = SupplyChainDB()\n",
    "\n",
    "# Load baseline metrics\n",
    "with open('data/processed/baseline_metrics.json', 'r') as f: # Nota: quit√© el '../' porque ya estamos en root\n",
    "    baseline = json.load(f)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TECHGEAR SUPPLY CHAIN - DATA EXPLORATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBaseline Metrics:\")\n",
    "for key, value in baseline.items():\n",
    "    if 'pct' in key:\n",
    "        print(f\"  {key:30s}: {value:.2f}%\")\n",
    "    elif isinstance(value, float):\n",
    "        print(f\"  {key:30s}: ${value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key:30s}: {value:,}\")\n",
    "        \n",
    "        \n",
    "# %% [markdown]\n",
    "# # 1. Revenue Trend Analysis\n",
    "# \n",
    "# Validating 15% YoY growth and identifying patterns\n",
    "\n",
    "# %% Revenue over time\n",
    "revenue_daily = db.query(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        SUM(revenue) as daily_revenue,\n",
    "        SUM(units_sold) as daily_units,\n",
    "        COUNT(*) as num_transactions\n",
    "    FROM sales\n",
    "    GROUP BY date\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "# Calculate 7-day moving average\n",
    "revenue_daily['revenue_7d_ma'] = revenue_daily['daily_revenue'].rolling(7).mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=revenue_daily['date'],\n",
    "    y=revenue_daily['daily_revenue'],\n",
    "    mode='lines',\n",
    "    name='Daily Revenue',\n",
    "    line=dict(color='lightblue', width=1),\n",
    "    opacity=0.5\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=revenue_daily['date'],\n",
    "    y=revenue_daily['revenue_7d_ma'],\n",
    "    mode='lines',\n",
    "    name='7-Day Moving Average',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Daily Revenue Trend (2023-2024)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Revenue ($)',\n",
    "    hovermode='x unified',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate growth\n",
    "revenue_daily['year'] = pd.to_datetime(revenue_daily['date']).dt.year\n",
    "yearly = revenue_daily.groupby('year')['daily_revenue'].sum()\n",
    "growth_rate = (yearly[2024] / yearly[2023] - 1) * 100\n",
    "\n",
    "print(f\"\\nüìà Year-over-Year Growth: {growth_rate:.2f}%\")\n",
    "print(f\"   2023 Total Revenue: ${yearly[2023]:,.2f}\")\n",
    "print(f\"   2024 Total Revenue: ${yearly[2024]:,.2f}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # 2. Seasonality Analysis\n",
    "# \n",
    "# Identifying peak seasons and demand patterns\n",
    "\n",
    "# %% Monthly seasonality\n",
    "revenue_daily['month'] = pd.to_datetime(revenue_daily['date']).dt.month\n",
    "revenue_daily['year'] = pd.to_datetime(revenue_daily['date']).dt.year\n",
    "\n",
    "monthly_by_year = revenue_daily.groupby(['year', 'month'])['daily_revenue'].mean().reset_index()\n",
    "\n",
    "fig = px.line(\n",
    "    monthly_by_year,\n",
    "    x='month',\n",
    "    y='daily_revenue',\n",
    "    color='year',\n",
    "    title='Average Daily Revenue by Month',\n",
    "    labels={'daily_revenue': 'Avg Daily Revenue ($)', 'month': 'Month'},\n",
    "    markers=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickmode='array',\n",
    "    tickvals=list(range(1, 13)),\n",
    "    ticktext=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Peak months\n",
    "monthly_avg = revenue_daily.groupby('month')['daily_revenue'].mean()\n",
    "peak_month = monthly_avg.idxmax()\n",
    "lowest_month = monthly_avg.idxmin()\n",
    "peak_ratio = monthly_avg.max() / monthly_avg.min()\n",
    "\n",
    "print(f\"\\nüìä Seasonality Metrics:\")\n",
    "print(f\"   Peak month: {peak_month} (${monthly_avg.max():,.2f}/day)\")\n",
    "print(f\"   Lowest month: {lowest_month} (${monthly_avg.min():,.2f}/day)\")\n",
    "print(f\"   Peak/Low ratio: {peak_ratio:.2f}x\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3. External Events Impact\n",
    "# \n",
    "# Quantifying the effect of Black Friday, supply disruptions, etc.\n",
    "\n",
    "# %% Events impact analysis\n",
    "events = db.query(\"SELECT * FROM external_events\")\n",
    "sales_with_events = db.query(\"\"\"\n",
    "    SELECT \n",
    "        s.date,\n",
    "        SUM(s.revenue) as daily_revenue,\n",
    "        e.type as event_type,\n",
    "        e.description\n",
    "    FROM sales s\n",
    "    LEFT JOIN external_events e \n",
    "        ON s.date >= CAST(e.date AS DATE)\n",
    "        AND s.date <= CAST(e.date AS DATE) + INTERVAL (e.duration_days) DAY\n",
    "    GROUP BY s.date, e.type, e.description\n",
    "    ORDER BY s.date\n",
    "\"\"\")\n",
    "\n",
    "# Focus on major events\n",
    "major_events = {\n",
    "    'Black-Friday': ('2023-11-24', 4),\n",
    "    'Back-to-School': ('2023-08-01', 45),\n",
    "    'Supply-Disruption': ('2024-03-10', 21)\n",
    "}\n",
    "\n",
    "for event_name, (start_date, duration) in major_events.items():\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = start + pd.Timedelta(days=duration)\n",
    "    \n",
    "    event_revenue = revenue_daily[\n",
    "        (revenue_daily['date'] >= start) & \n",
    "        (revenue_daily['date'] <= end)\n",
    "    ]['daily_revenue'].mean()\n",
    "    \n",
    "    # Baseline: 2 weeks before event\n",
    "    baseline_start = start - pd.Timedelta(days=14)\n",
    "    baseline_end = start - pd.Timedelta(days=1)\n",
    "    baseline_revenue = revenue_daily[\n",
    "        (revenue_daily['date'] >= baseline_start) & \n",
    "        (revenue_daily['date'] <= baseline_end)\n",
    "    ]['daily_revenue'].mean()\n",
    "    \n",
    "    impact = (event_revenue / baseline_revenue - 1) * 100\n",
    "    \n",
    "    print(f\"\\n{event_name}:\")\n",
    "    print(f\"   Baseline (2 weeks prior): ${baseline_revenue:,.2f}/day\")\n",
    "    print(f\"   During event: ${event_revenue:,.2f}/day\")\n",
    "    print(f\"   Impact: {impact:+.1f}%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# %% [markdown]\n",
    "# # 4. Product Performance Analysis\n",
    "# \n",
    "# Which products drive revenue and which have issues?\n",
    "\n",
    "# %% Top products by revenue\n",
    "top_products = db.query(\"\"\"\n",
    "    SELECT \n",
    "        p.product_id,\n",
    "        p.name,\n",
    "        p.category,\n",
    "        p.tier,\n",
    "        SUM(s.revenue) as total_revenue,\n",
    "        SUM(s.units_sold) as total_units,\n",
    "        SUM(s.profit) as total_profit,\n",
    "        COUNT(DISTINCT s.date) as days_with_sales,\n",
    "        AVG(s.unit_price) as avg_price\n",
    "    FROM sales s\n",
    "    JOIN products p ON s.product_id = p.product_id\n",
    "    GROUP BY p.product_id, p.name, p.category, p.tier\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "fig = px.bar(\n",
    "    top_products,\n",
    "    x='name',\n",
    "    y='total_revenue',\n",
    "    color='category',\n",
    "    title='Top 20 Products by Revenue',\n",
    "    labels={'total_revenue': 'Total Revenue ($)', 'name': 'Product'},\n",
    "    text='total_revenue'\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\n",
    "fig.update_layout(xaxis_tickangle=-45, height=600)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüèÜ Top 5 Products:\")\n",
    "print(top_products[['product_id', 'name', 'total_revenue']].head().to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # 5. Stockout Analysis\n",
    "# \n",
    "# **CRITICAL**: Where is the baseline system failing?\n",
    "\n",
    "# %% Stockouts by product\n",
    "stockout_analysis = db.query(\"\"\"\n",
    "    SELECT \n",
    "        i.product_id,\n",
    "        p.name,\n",
    "        p.category,\n",
    "        COUNT(*) as total_days,\n",
    "        SUM(CASE WHEN i.stockout = 1 THEN 1 ELSE 0 END) as stockout_days,\n",
    "        SUM(CASE WHEN i.stockout = 1 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as stockout_pct,\n",
    "        AVG(i.units_on_hand) as avg_inventory,\n",
    "        p.unit_price * p.base_demand_daily as daily_revenue_potential\n",
    "    FROM inventory_snapshots i\n",
    "    JOIN products p ON i.product_id = p.product_id\n",
    "    GROUP BY i.product_id, p.name, p.category, p.unit_price, p.base_demand_daily\n",
    "    HAVING stockout_pct > 0\n",
    "    ORDER BY stockout_days DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "fig = px.bar(\n",
    "    stockout_analysis,\n",
    "    x='name',\n",
    "    y='stockout_days',\n",
    "    color='category',\n",
    "    title='Top 20 Products by Stockout Days',\n",
    "    labels={'stockout_days': 'Days Out of Stock', 'name': 'Product'},\n",
    "    text='stockout_pct'\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')\n",
    "fig.update_layout(xaxis_tickangle=-45, height=600)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Worst Stockout Products:\")\n",
    "print(stockout_analysis[['product_id', 'name', 'stockout_days', 'stockout_pct']].head(10).to_string(index=False))\n",
    "\n",
    "# Calculate lost revenue\n",
    "stockout_analysis['estimated_lost_revenue'] = (\n",
    "    stockout_analysis['stockout_days'] * \n",
    "    stockout_analysis['daily_revenue_potential']\n",
    ")\n",
    "\n",
    "total_lost = stockout_analysis['estimated_lost_revenue'].sum()\n",
    "print(f\"\\nüí∞ Estimated Lost Revenue from Stockouts: ${total_lost:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# %% Stockouts by warehouse\n",
    "warehouse_stockouts = db.query(\"\"\"\n",
    "    SELECT \n",
    "        i.warehouse_id,\n",
    "        COUNT(*) as total_snapshots,\n",
    "        SUM(CASE WHEN i.stockout = 1 THEN 1 ELSE 0 END) as stockout_count,\n",
    "        SUM(CASE WHEN i.stockout = 1 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as stockout_pct\n",
    "    FROM inventory_snapshots i\n",
    "    GROUP BY i.warehouse_id\n",
    "    ORDER BY stockout_pct DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüè≠ Stockouts by Warehouse:\")\n",
    "print(warehouse_stockouts.to_string(index=False))\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # 6. Purchase Order Analysis\n",
    "# \n",
    "# Understanding baseline ordering behavior\n",
    "\n",
    "# %% Orders over time\n",
    "orders_by_date = db.query(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', order_date) as month,\n",
    "        COUNT(*) as num_orders,\n",
    "        SUM(units_ordered) as total_units,\n",
    "        SUM(total_cost) as total_cost,\n",
    "        AVG(total_cost) as avg_order_value,\n",
    "        SUM(CASE WHEN volume_discount_applied THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as discount_pct\n",
    "    FROM purchase_orders\n",
    "    GROUP BY month\n",
    "    ORDER BY month\n",
    "\"\"\")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=('Purchase Orders Over Time', 'Volume Discount Capture Rate'),\n",
    "    vertical_spacing=0.12\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=orders_by_date['month'], y=orders_by_date['num_orders'], name='Orders'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=orders_by_date['month'], y=orders_by_date['discount_pct'], \n",
    "               name='Discount %', mode='lines+markers'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Month\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Number of Orders\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Discount Capture %\", row=2, col=1)\n",
    "fig.update_layout(height=700, showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# %% Supplier performance comparison\n",
    "supplier_performance = db.query(\"\"\"\n",
    "    SELECT \n",
    "        po.supplier_id,\n",
    "        s.name as supplier_name,\n",
    "        s.lead_time_days as expected_lead_time,\n",
    "        s.reliability_score as expected_reliability,\n",
    "        COUNT(*) as num_orders,\n",
    "        SUM(po.total_cost) as total_spend,\n",
    "        AVG(po.lead_time_actual) as avg_actual_lead_time,\n",
    "        AVG(po.lead_time_expected) as avg_expected_lead_time,\n",
    "        SUM(CASE WHEN po.on_time THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as actual_on_time_pct,\n",
    "        SUM(CASE WHEN po.volume_discount_applied THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as discount_rate,\n",
    "        AVG(po.shipping_cost) as avg_shipping_cost\n",
    "    FROM purchase_orders po\n",
    "    JOIN suppliers s ON po.supplier_id = s.supplier_id\n",
    "    GROUP BY po.supplier_id, s.name, s.lead_time_days, s.reliability_score\n",
    "    ORDER BY total_spend DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüöö Supplier Performance:\")\n",
    "print(supplier_performance.to_string(index=False))\n",
    "\n",
    "# Highlight discrepancies\n",
    "print(\"\\n‚ö†Ô∏è  Reliability Gaps:\")\n",
    "for _, row in supplier_performance.iterrows():\n",
    "    gap = row['actual_on_time_pct'] - (row['expected_reliability'] * 100)\n",
    "    if abs(gap) > 5:\n",
    "        print(f\"   {row['supplier_id']}: Expected {row['expected_reliability']*100:.1f}%, \"\n",
    "              f\"Actual {row['actual_on_time_pct']:.1f}% (gap: {gap:+.1f}pp)\")\n",
    "        \n",
    "        \n",
    "        \n",
    "# %% [markdown]\n",
    "# # 7. Key Findings Summary\n",
    "\n",
    "# Calcular ingresos por categor√≠a para el resumen final\n",
    "category_revenue = db.query(\"\"\"\n",
    "    SELECT \n",
    "        p.category,\n",
    "        SUM(s.revenue) as total_revenue\n",
    "    FROM sales s\n",
    "    JOIN products p ON s.product_id = p.product_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS - DATA EXPLORATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä REVENUE PATTERNS:\")\n",
    "print(f\"   ‚Ä¢ YoY Growth: {growth_rate:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Seasonality: {peak_ratio:.2f}x variation (peak vs low)\")\n",
    "print(f\"   ‚Ä¢ Top category: {category_revenue.iloc[0]['category']} \"\n",
    "      f\"(${category_revenue.iloc[0]['total_revenue']:,.2f})\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  OPERATIONAL ISSUES:\")\n",
    "print(f\"   ‚Ä¢ Stockout rate: {baseline['stockout_rate_pct']:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Lost revenue: ${baseline['estimated_lost_revenue']:,.2f}\")\n",
    "print(f\"   ‚Ä¢ Products with stockouts: {len(stockout_analysis)}\")\n",
    "print(f\"   ‚Ä¢ Worst product: {stockout_analysis.iloc[0]['name']} \"\n",
    "      f\"({stockout_analysis.iloc[0]['stockout_days']:.0f} days)\")\n",
    "\n",
    "print(f\"\\nüí∞ PROCUREMENT:\")\n",
    "print(f\"   ‚Ä¢ Total spend: ${baseline['total_procurement']:,.2f}\")\n",
    "print(f\"   ‚Ä¢ Discount capture: {baseline['discount_capture_pct']:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Average order: ${baseline['avg_order_value']:,.2f}\")\n",
    "print(f\"   ‚Ä¢ Total orders: {baseline['total_orders']:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT: Analyze specific failure scenarios\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
